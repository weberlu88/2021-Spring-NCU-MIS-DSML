{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy 106403551 of Data Science and Machine Learning pretest v21.02.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4MpIDbALpZ9"
      },
      "source": [
        "# 2021 Data Science and Machine Learning Pretest\r\n",
        "\r\n",
        "*   This Colab is read-only. Please save a copy of it on your Drive to edit it by going to `Menu > File > Save a copy in Drive`.\r\n",
        "*   Rename your Colab in the following format and replace 109423000 with your student ID:\r\n",
        "> `Copy 109423000 of Data Science and Machine Learning pretest v21.02.ipynb`\r\n",
        "*   You are required to complete this pretest **on your own**.\r\n",
        "*   When you have completed the questions below, make sure you turn on the **share/edit/view persmission**.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOY37mmch2f9"
      },
      "source": [
        "# Question 1: Text preprocessing\n",
        "Most of the text data acquired through web crawling and review can be noisy. When handling this kind of text data, preprocessing is an important step to ensure the quality of the dataset. There are multiple ways of doing text preprocessing. Below is an example flow of preprocessing text data. \n",
        "\n",
        "1. lowercase \n",
        "2. decontracting \n",
        "3. remove tags, punctuations, numbers\n",
        "3. tokenization \n",
        "4. stopword removal \n",
        "5. lemmatization \n",
        "6. stemming\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "source": [
        "## 1-1. Please briefly explain what each step is doing.(30%)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "\n",
        "1. Standardize text data in single format. Since the word 'love' is same as 'LoVE'.\n",
        "2. English has a couple of contractions(abbreviations/縮寫). For instance 'aren't' stands for 'are not'. We should split/decontract contractions in full sentence in case of consistency(一致性).\n",
        "3. In many cases, we want to remove all the non-words charactersthe (e.g. punctuation marks, HTML tags) and it’s easy to remove them with regex.\n",
        "4. Since the data all we have are sentences. But the basic document unit are 'tokens', not 'sentences'. A *token* is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing.\n",
        "    - *Sentence tokenization* (also called sentence segmentation) is the problem of dividing a string of written language into its component sentences.\n",
        "    - *Word tokenization* (also called word segmentation) is the problem of dividing a string of written language into its component words.\n",
        "5. Stop words are words which are **filtered out** before or after processing of text. They are the words in any language which does not add much meaning to a sentence. Stopwords could cause noise, that’s why we want to remove these irrelevant words. Common stop words: “and”, “the”, “a”\n",
        "\n",
        "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:\n",
        "```js\n",
        "am, are, is => be //統一使用字根\n",
        "car, cars, car's, cars' => car //去詞類變化\n",
        "```\n",
        "The result of this mapping of text will be something like:\n",
        "```\n",
        "the boy's cars are different colors =>\n",
        "the boy car be differ color\n",
        "```\n",
        "\n",
        "6. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words(基於語言學/字典定義), normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .\n",
        "7. Stemmers use language-specific rules, but they require less knowledge than a lemmatizer. It usually refers to a crude heuristic process(猜字根) that chops off the ends of words, which increases recall while harming precision. \n",
        "\n",
        "References:\n",
        "- https://nlp.stanford.edu/IR-book/html/htmledition/contents-1.html\n",
        "- https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63\n",
        "- https://medium.com/analytics-vidhya/natural-language-processing-for-developers-912ee0fda979 (with github code)\n",
        "- Tokenization https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/\n",
        "- Stop words https://medium.com/@saitejaponugoti/stop-words-in-nlp-5b248dadad47#:~:text=In%20computing%2C%20stop%20words%20are,universal%20list%20of%20stop%20words.\n",
        "- [Youtube | Natural Language Processing In 10 Minutes](https://www.youtube.com/watch?v=5ctbvkAMQO4)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP3t4W6flEIC"
      },
      "source": [
        "## 1-2. Please use the sample data and do the preprocessing following the provided flow.(70%)"
      ]
    },
    {
      "source": [
        "documents = [\"Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as 'Jumbo'\",\r\n",
        "        \"I WAS VISITING MY FRIEND NATE THE OTHER MORNING FOR COFFEE , HE CAME OUT OF HIS STORAGE ROOM WITH ( A PACKET OF McCANNS INSTANT IRISH OATMEAL .) HE SUGGESTED THAT I TRY IT FOR MY OWN USE ,IN MY STASH . SOMETIMES NATE DOSE NOT GIVE YOU A CHANCE TO SAY NO , SO I ENDED UP TRYING THE APPLE AND CINN . FOUND IT TO BE VERY TASTEFULL WHEN MADE WITH WATER OR POWDERED MILK . IT GOES GOOD WITH O.J. AND COFFEE AND A SLICE OF TOAST AND YOUR READY TO TAKE ON THE WORLD...OR THE DAY AT LEAST..  JERRY REITH...\",\r\n",
        "        \"I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind!  We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away!  When we realized that we simply couldn't find it anywhere in our city we were bummed.<br /><br />Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.<br /><br />If you love hot sauce..I mean really love hot sauce, but don't want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan.  Just realize that once you taste it, you will never want to use any other sauce.<br /><br />Thank you for the personal, incredible service!\",\r\n",
        "        \"Product received is as advertised.<br /><br /><a href='http://www.amazon.com/gp/product/B001GVISJM'>Twizzlers, Strawberry, 16-Ounce Bags (Pack of 6)</a>\",\r\n",
        "        \"this was sooooo deliscious but too bad i ate em too fast and gained 2 pds! my fault\",\r\n",
        "        \"Deal was awesome!  Arrived before Halloween as indicated and was enough to satisfy trick or treaters.  I love the quality of this product and it was much less expensive than the local store's candy.\",\r\n",
        "        \"I love these.........very tasty!!!!!!!!!!!  Infact, I think I am addicted to them.<br />Buying them in packs of 6 bags - is very reasonable than going to Target and getting a bag.  Savings are about a $1.00 a bag.  I use subscribe and save on these and several other product.  I love subscribe and save!!!!!!!!!!!\",\r\n",
        "        \"I LOVE spicy ramen, but for whatever reasons this thing burns my stomach badly and the burning sensation doesn't go away for like 3 hours! Not sure if that is healthy or not .... and you can buy this at Walmart for $0.28, way cheaper than Amazon.\",\r\n",
        "        \"Makes a tasty, super easy meal, fast. BUT high in calories.<br /><br />The instructions say to saute the veggies first but I recommend cooking the chicken first. The chicken takes longer to cook and the raw chicken ontop of veggies just makes a slimy mess. I made it with snow peas and carrots only. I dont like the little corn.  Added some red pepper flakes for heat and served ontop of rice.  It came out wonderful! Dinner on the table in less than 30mins.\",\r\n",
        "        \"Love this sugar.  I also get muscavado sugar and they are both great to use in place of regular white sugar. Recommend!\",\r\n",
        "        \"This is just Fantastic Chicken Noodle soup, the best I have ever eaten, with large hearty chunks of chicken,and vegetables and nice large noodles. This soup is just so full bodied, and is seasoned just right.  I am so glad Amazon carries this product.  I just can't find it here in Vermont.\"]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "y7n8-qVplIrU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "LLEiWlmoqmpY",
        "outputId": "b3d26426-9cc0-4241-df7b-edc17b98e7f5"
      },
      "source": [
        "import pandas as pd\r\n",
        "df= pd.DataFrame(documents,columns=['sentence'])\r\n",
        "df"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             sentence\n",
              "0   Product arrived labeled as Jumbo Salted Peanut...\n",
              "1   I WAS VISITING MY FRIEND NATE THE OTHER MORNIN...\n",
              "2   I don't know if it's the cactus or the tequila...\n",
              "3   Product received is as advertised.<br /><br />...\n",
              "4   this was sooooo deliscious but too bad i ate e...\n",
              "5   Deal was awesome!  Arrived before Halloween as...\n",
              "6   I love these.........very tasty!!!!!!!!!!!  In...\n",
              "7   I LOVE spicy ramen, but for whatever reasons t...\n",
              "8   Makes a tasty, super easy meal, fast. BUT high...\n",
              "9   Love this sugar.  I also get muscavado sugar a...\n",
              "10  This is just Fantastic Chicken Noodle soup, th..."
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>I WAS VISITING MY FRIEND NATE THE OTHER MORNIN...</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>I don't know if it's the cactus or the tequila...</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>Product received is as advertised.&lt;br /&gt;&lt;br /&gt;...</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>this was sooooo deliscious but too bad i ate e...</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>Deal was awesome!  Arrived before Halloween as...</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>I love these.........very tasty!!!!!!!!!!!  In...</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>I LOVE spicy ramen, but for whatever reasons t...</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>Makes a tasty, super easy meal, fast. BUT high...</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>Love this sugar.  I also get muscavado sugar a...</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>This is just Fantastic Chicken Noodle soup, th...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Weber\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# decontracting\n",
        "def decontract(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "# remove non-alpha/number charater (Punctuation Marks)\n",
        "def text(phrase):\n",
        "    # By Python definition '\\W == [^a-zA-Z0-9_], which excludes all numbers, letters and _ (but we need to include whitespace \\s)\n",
        "    phrase = re.sub(r'[^a-zA-Z0-9_\\s]+', '', phrase)\n",
        "    return phrase\n",
        "\n",
        "# stopword removal & tokenization\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "# nltk.download('stopwords') # must download at first execution\n",
        "# nltk.download('punkt')\n",
        "\n",
        "def tokenize_rm_stopwords(phrase): \n",
        "    text_tokens = word_tokenize(phrase)\n",
        "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('english')]\n",
        "    return tokens_without_sw\n",
        "\n",
        "# stem/lemmatize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "# nltk.download('wordnet') \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize(token_lst):\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in token_lst])\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh man this is pretty cool We will do more such things\n['Oh', 'man', 'pretty', 'cool', 'We', 'things']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Oh man pretty cool We thing'"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# Unit test\n",
        "sample_text = \"Oh man, this's pretty cool. We will do more such things.\"\n",
        "res = decontract(sample_text)\n",
        "res = text(res)\n",
        "print(res)\n",
        "res = tokenize_rm_stopwords(res) # ㄚㄚㄚ砍太多字惹吧\n",
        "print(res)\n",
        "res = lemmatize(res)\n",
        "\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# modulize, can't change order\n",
        "def process_script(phrase):\n",
        "    phrase = phrase.lower()\n",
        "    res = decontract(phrase)\n",
        "    res = text(res)\n",
        "    res = tokenize_rm_stopwords(res)\n",
        "    res = lemmatize(res)\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             sentence  \\\n",
              "0   Product arrived labeled as Jumbo Salted Peanut...   \n",
              "1   I WAS VISITING MY FRIEND NATE THE OTHER MORNIN...   \n",
              "2   I don't know if it's the cactus or the tequila...   \n",
              "3   Product received is as advertised.<br /><br />...   \n",
              "4   this was sooooo deliscious but too bad i ate e...   \n",
              "5   Deal was awesome!  Arrived before Halloween as...   \n",
              "6   I love these.........very tasty!!!!!!!!!!!  In...   \n",
              "7   I LOVE spicy ramen, but for whatever reasons t...   \n",
              "8   Makes a tasty, super easy meal, fast. BUT high...   \n",
              "9   Love this sugar.  I also get muscavado sugar a...   \n",
              "10  This is just Fantastic Chicken Noodle soup, th...   \n",
              "\n",
              "                                            processed  \n",
              "0   product arrived labeled jumbo salted peanutsth...  \n",
              "1   visiting friend nate morning coffee came stora...  \n",
              "2   know cactus tequila unique combination ingredi...  \n",
              "3   product received advertisedbr br hrefhttpwwwam...  \n",
              "4   sooooo deliscious bad ate em fast gained 2 pd ...  \n",
              "5   deal awesome arrived halloween indicated enoug...  \n",
              "6   love thesevery tasty infact think addicted the...  \n",
              "7   love spicy ramen whatever reason thing burn st...  \n",
              "8   make tasty super easy meal fast high caloriesb...  \n",
              "9   love sugar also get muscavado sugar great use ...  \n",
              "10  fantastic chicken noodle soup best ever eaten ...  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>processed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n      <td>product arrived labeled jumbo salted peanutsth...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>I WAS VISITING MY FRIEND NATE THE OTHER MORNIN...</td>\n      <td>visiting friend nate morning coffee came stora...</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>I don't know if it's the cactus or the tequila...</td>\n      <td>know cactus tequila unique combination ingredi...</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>Product received is as advertised.&lt;br /&gt;&lt;br /&gt;...</td>\n      <td>product received advertisedbr br hrefhttpwwwam...</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>this was sooooo deliscious but too bad i ate e...</td>\n      <td>sooooo deliscious bad ate em fast gained 2 pd ...</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>Deal was awesome!  Arrived before Halloween as...</td>\n      <td>deal awesome arrived halloween indicated enoug...</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>I love these.........very tasty!!!!!!!!!!!  In...</td>\n      <td>love thesevery tasty infact think addicted the...</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>I LOVE spicy ramen, but for whatever reasons t...</td>\n      <td>love spicy ramen whatever reason thing burn st...</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>Makes a tasty, super easy meal, fast. BUT high...</td>\n      <td>make tasty super easy meal fast high caloriesb...</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>Love this sugar.  I also get muscavado sugar a...</td>\n      <td>love sugar also get muscavado sugar great use ...</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>This is just Fantastic Chicken Noodle soup, th...</td>\n      <td>fantastic chicken noodle soup best ever eaten ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# show results\n",
        "df['processed'] = df.apply(lambda row: process_script(row['sentence']), axis = 1)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a8slpNZh_ID"
      },
      "source": [
        "# Question 2: DataFrame handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGkOfX9LiF4o"
      },
      "source": [
        "\r\n",
        "\r\n",
        "*   Please download the datasets from the following link, https://www.kaggle.com/aaron7sun/stocknews\r\n",
        "*   Save the downloaded files on your own drive and load the file for later use.\r\n",
        "- There are two channels of data provided in this dataset:\r\n",
        "\r\n",
        "  - **News data:** Crawled historical news headlines from Reddit WorldNews Channel . They are ranked by reddit users' votes, and only the top 25 headlines are considered for a single date. (Range: 2008-06-08 to 2016-07-01)\r\n",
        "\r\n",
        "  - **Stock data:** Dow Jones Industrial Average (DJIA) is used to \"prove the concept\". (Range: 2008-08-08 to 2016-07-01)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaguSDD3iO_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1477f364-62ce-4d94-b96d-782118f8778d"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX970ixtiPnh"
      },
      "source": [
        "import pandas as pd\r\n",
        "corpus_root = 'drive/My Drive/Colab Notebooks/datasets/' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load local file (optional)\n",
        "# date type : <class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
        "import pandas as pd\n",
        "news_df = pd.read_csv('RedditNews.csv', parse_dates =[\"Date\"])\n",
        "stock_df = pd.read_csv('upload_DJIA_table.csv', parse_dates =[\"Date\"], index_col =\"Date\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHMR3Rv5iRV4"
      },
      "source": [
        "news_df = pd.read_csv(corpus_root+'RedditNews.csv')\n",
        "stock_df = pd.read_csv(corpus_root+'upload_DJIA_table.csv')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'corpus_root' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-2-28098802dbc5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnews_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_root\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'RedditNews.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mstock_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_root\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'upload_DJIA_table.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'corpus_root' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFqPUNxWkK-K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4b86fde3-3c17-45bf-8797-ad31877db4cc"
      },
      "source": [
        "news_df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Date                                               News\n",
              "0 2016-07-01  A 117-year-old woman in Mexico City finally re...\n",
              "1 2016-07-01   IMF chief backs Athens as permanent Olympic host\n",
              "2 2016-07-01  The president of France says if Brexit won, so...\n",
              "3 2016-07-01  British Man Who Must Give Police 24 Hours' Not...\n",
              "4 2016-07-01  100+ Nobel laureates urge Greenpeace to stop o..."
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>News</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2016-07-01</td>\n      <td>A 117-year-old woman in Mexico City finally re...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2016-07-01</td>\n      <td>IMF chief backs Athens as permanent Olympic host</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2016-07-01</td>\n      <td>The president of France says if Brexit won, so...</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2016-07-01</td>\n      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2016-07-01</td>\n      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smpTiq1DiST4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "6093d5c4-cd49-42e0-cb65-ebfbe47b81e5"
      },
      "source": [
        "stock_df.head(15)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    Open          High           Low         Close     Volume  \\\n",
              "Date                                                                            \n",
              "2016-07-01  17924.240234  18002.380859  17916.910156  17949.369141   82160000   \n",
              "2016-06-30  17712.759766  17930.609375  17711.800781  17929.990234  133030000   \n",
              "2016-06-29  17456.019531  17704.509766  17456.019531  17694.679688  106380000   \n",
              "2016-06-28  17190.509766  17409.720703  17190.509766  17409.720703  112190000   \n",
              "2016-06-27  17355.210938  17355.210938  17063.080078  17140.240234  138740000   \n",
              "2016-06-24  17946.630859  17946.630859  17356.339844  17400.750000  239000000   \n",
              "2016-06-23  17844.109375  18011.070312  17844.109375  18011.070312   98070000   \n",
              "2016-06-22  17832.669922  17920.160156  17770.359375  17780.830078   89440000   \n",
              "2016-06-21  17827.330078  17877.839844  17799.800781  17829.730469   85130000   \n",
              "2016-06-20  17736.869141  17946.359375  17736.869141  17804.869141   99380000   \n",
              "2016-06-17  17733.439453  17733.439453  17602.779297  17675.160156  248680000   \n",
              "2016-06-16  17602.230469  17754.910156  17471.289062  17733.099609   91950000   \n",
              "2016-06-15  17703.650391  17762.960938  17629.009766  17640.169922   94130000   \n",
              "2016-06-14  17710.769531  17733.919922  17595.789062  17674.820312   93740000   \n",
              "2016-06-13  17830.500000  17893.279297  17731.349609  17732.480469  101690000   \n",
              "\n",
              "               Adj Close  \n",
              "Date                      \n",
              "2016-07-01  17949.369141  \n",
              "2016-06-30  17929.990234  \n",
              "2016-06-29  17694.679688  \n",
              "2016-06-28  17409.720703  \n",
              "2016-06-27  17140.240234  \n",
              "2016-06-24  17400.750000  \n",
              "2016-06-23  18011.070312  \n",
              "2016-06-22  17780.830078  \n",
              "2016-06-21  17829.730469  \n",
              "2016-06-20  17804.869141  \n",
              "2016-06-17  17675.160156  \n",
              "2016-06-16  17733.099609  \n",
              "2016-06-15  17640.169922  \n",
              "2016-06-14  17674.820312  \n",
              "2016-06-13  17732.480469  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n      <th>Adj Close</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2016-07-01</td>\n      <td>17924.240234</td>\n      <td>18002.380859</td>\n      <td>17916.910156</td>\n      <td>17949.369141</td>\n      <td>82160000</td>\n      <td>17949.369141</td>\n    </tr>\n    <tr>\n      <td>2016-06-30</td>\n      <td>17712.759766</td>\n      <td>17930.609375</td>\n      <td>17711.800781</td>\n      <td>17929.990234</td>\n      <td>133030000</td>\n      <td>17929.990234</td>\n    </tr>\n    <tr>\n      <td>2016-06-29</td>\n      <td>17456.019531</td>\n      <td>17704.509766</td>\n      <td>17456.019531</td>\n      <td>17694.679688</td>\n      <td>106380000</td>\n      <td>17694.679688</td>\n    </tr>\n    <tr>\n      <td>2016-06-28</td>\n      <td>17190.509766</td>\n      <td>17409.720703</td>\n      <td>17190.509766</td>\n      <td>17409.720703</td>\n      <td>112190000</td>\n      <td>17409.720703</td>\n    </tr>\n    <tr>\n      <td>2016-06-27</td>\n      <td>17355.210938</td>\n      <td>17355.210938</td>\n      <td>17063.080078</td>\n      <td>17140.240234</td>\n      <td>138740000</td>\n      <td>17140.240234</td>\n    </tr>\n    <tr>\n      <td>2016-06-24</td>\n      <td>17946.630859</td>\n      <td>17946.630859</td>\n      <td>17356.339844</td>\n      <td>17400.750000</td>\n      <td>239000000</td>\n      <td>17400.750000</td>\n    </tr>\n    <tr>\n      <td>2016-06-23</td>\n      <td>17844.109375</td>\n      <td>18011.070312</td>\n      <td>17844.109375</td>\n      <td>18011.070312</td>\n      <td>98070000</td>\n      <td>18011.070312</td>\n    </tr>\n    <tr>\n      <td>2016-06-22</td>\n      <td>17832.669922</td>\n      <td>17920.160156</td>\n      <td>17770.359375</td>\n      <td>17780.830078</td>\n      <td>89440000</td>\n      <td>17780.830078</td>\n    </tr>\n    <tr>\n      <td>2016-06-21</td>\n      <td>17827.330078</td>\n      <td>17877.839844</td>\n      <td>17799.800781</td>\n      <td>17829.730469</td>\n      <td>85130000</td>\n      <td>17829.730469</td>\n    </tr>\n    <tr>\n      <td>2016-06-20</td>\n      <td>17736.869141</td>\n      <td>17946.359375</td>\n      <td>17736.869141</td>\n      <td>17804.869141</td>\n      <td>99380000</td>\n      <td>17804.869141</td>\n    </tr>\n    <tr>\n      <td>2016-06-17</td>\n      <td>17733.439453</td>\n      <td>17733.439453</td>\n      <td>17602.779297</td>\n      <td>17675.160156</td>\n      <td>248680000</td>\n      <td>17675.160156</td>\n    </tr>\n    <tr>\n      <td>2016-06-16</td>\n      <td>17602.230469</td>\n      <td>17754.910156</td>\n      <td>17471.289062</td>\n      <td>17733.099609</td>\n      <td>91950000</td>\n      <td>17733.099609</td>\n    </tr>\n    <tr>\n      <td>2016-06-15</td>\n      <td>17703.650391</td>\n      <td>17762.960938</td>\n      <td>17629.009766</td>\n      <td>17640.169922</td>\n      <td>94130000</td>\n      <td>17640.169922</td>\n    </tr>\n    <tr>\n      <td>2016-06-14</td>\n      <td>17710.769531</td>\n      <td>17733.919922</td>\n      <td>17595.789062</td>\n      <td>17674.820312</td>\n      <td>93740000</td>\n      <td>17674.820312</td>\n    </tr>\n    <tr>\n      <td>2016-06-13</td>\n      <td>17830.500000</td>\n      <td>17893.279297</td>\n      <td>17731.349609</td>\n      <td>17732.480469</td>\n      <td>101690000</td>\n      <td>17732.480469</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H03CMsosjphW"
      },
      "source": [
        "\r\n",
        "## 2-1. Define a *funtion* aims to know the weekly (7 days) Stock **Close value** trend. (80%)\r\n",
        "- Implement a *funtion* that determines the weekly (7 days) Stock **Close value** trend using the Stock data and **then record the trend in the News data in a new column \"label\" with 0, 1 and -1**. For example, \r\n",
        "  - On 2016-06-13, the market closed at 17732.480469. Seven days later, 2016-06-20, the market closed **higher** at 17804.869141. In this scenario, all entries corresponding to 2016-06-13 in the News data will be **marked 1** in the \"label\" column. Dates 2016-06-14 and 2016-06-15 will also be marked 1 because 2016-06-21 and 2016-06-22 closed higher, respectively.\r\n",
        "  - On the other hand, 2016-06-17 will be **marked 0** because 2016-06-24 (7 days later) closed **lower**.\r\n",
        "  - If a given date does not have a corresponding date for 7 days later, the given date will be **marked -1**.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yf_CKVXkNy8"
      },
      "source": [
        "import datetime\n",
        "def label(date, close):\n",
        "  '''\n",
        "  please answer here, you can add any parameters if you want.\n",
        "  but don't import other libraries, this notebook already prepared the libraries which all you need !\n",
        "  remember that, the standard for evaluation include your:\n",
        "    1. Time Complexity (80%)\n",
        "    2. Program Logic (10%)\n",
        "    3. Creativity (10%)\n",
        "  '''\n",
        "  date_7d = date + pd.Timedelta(7, unit=\"d\")\n",
        "\n",
        "  try:\n",
        "    # find 7 days later, mark rises(1) and falls(0)\n",
        "    close_7d = stock_df.loc[date_7d]['Close']\n",
        "    price_delta = close_7d - close\n",
        "    # print(str(date)+\" -> \"+str(date_7d)+\", delta= \"+str(price_delta))\n",
        "    return 1 if (price_delta > 0) else 0\n",
        "  \n",
        "  except:\n",
        "    # there's no closing value 7 days later, mark as NaN (-1)\n",
        "    # print(\"except: \"+str(date_7d.date()))\n",
        "    return -1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUM9RQjkkOIk"
      },
      "source": [
        "'''\n",
        "than apply your defined function on stock_df here.\n",
        "return results store into a new columns name \"Label\"\n",
        "\n",
        "'''\n",
        "label_lst = []\n",
        "for index, row in stock_df.iterrows():\n",
        "    label_lst.append(label(index, row['Close']))\n",
        "    # print(index, row['Close'])\n",
        "\n",
        "# label_lst\n",
        "# stock_df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    Open          High           Low         Close     Volume  \\\n",
              "Date                                                                            \n",
              "2008-09-05  11185.629883  11245.150391  11037.849609  11220.959961  198300000   \n",
              "2008-09-04  11532.480469  11532.480469  11176.019531  11188.230469  229200000   \n",
              "2008-09-03  11506.009766  11554.379883  11416.530273  11532.879883  174250000   \n",
              "2008-09-02  11545.629883  11790.169922  11471.900391  11516.919922  177090000   \n",
              "2008-08-29  11713.230469  11713.230469  11543.389648  11543.959961  166910000   \n",
              "2008-08-28  11499.870117  11715.179688  11499.790039  11715.179688  149150000   \n",
              "2008-08-27  11412.459961  11554.459961  11381.769531  11502.509766  120580000   \n",
              "2008-08-26  11383.559570  11436.240234  11340.410156  11412.870117  119800000   \n",
              "2008-08-25  11626.190430  11626.269531  11362.629883  11386.250000  148610000   \n",
              "2008-08-22  11426.790039  11632.129883  11426.790039  11628.059570  138790000   \n",
              "2008-08-21  11415.230469  11476.209961  11315.570312  11430.209961  130020000   \n",
              "2008-08-20  11345.940430  11454.150391  11290.580078  11417.429688  144880000   \n",
              "2008-08-19  11478.089844  11478.169922  11318.500000  11348.549805  171580000   \n",
              "2008-08-18  11659.650391  11690.429688  11434.120117  11479.389648  156290000   \n",
              "2008-08-15  11611.209961  11709.889648  11599.730469  11659.900391  215040000   \n",
              "2008-08-14  11532.070312  11718.280273  11450.889648  11615.929688  159790000   \n",
              "2008-08-13  11632.809570  11633.780273  11453.339844  11532.959961  182550000   \n",
              "2008-08-12  11781.700195  11782.349609  11601.519531  11642.469727  173590000   \n",
              "2008-08-11  11729.669922  11867.110352  11675.530273  11782.349609  183190000   \n",
              "2008-08-08  11432.089844  11759.959961  11388.040039  11734.320312  212830000   \n",
              "\n",
              "               Adj Close  Label  \n",
              "Date                             \n",
              "2008-09-05  11220.959961      1  \n",
              "2008-09-04  11188.230469      1  \n",
              "2008-09-03  11532.879883      0  \n",
              "2008-09-02  11516.919922      0  \n",
              "2008-08-29  11543.959961      0  \n",
              "2008-08-28  11715.179688      0  \n",
              "2008-08-27  11502.509766      1  \n",
              "2008-08-26  11412.870117      1  \n",
              "2008-08-25  11386.250000     -1  \n",
              "2008-08-22  11628.059570      0  \n",
              "2008-08-21  11430.209961      1  \n",
              "2008-08-20  11417.429688      1  \n",
              "2008-08-19  11348.549805      1  \n",
              "2008-08-18  11479.389648      0  \n",
              "2008-08-15  11659.900391      0  \n",
              "2008-08-14  11615.929688      0  \n",
              "2008-08-13  11532.959961      0  \n",
              "2008-08-12  11642.469727      0  \n",
              "2008-08-11  11782.349609      0  \n",
              "2008-08-08  11734.320312      0  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Volume</th>\n      <th>Adj Close</th>\n      <th>Label</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>2008-09-05</td>\n      <td>11185.629883</td>\n      <td>11245.150391</td>\n      <td>11037.849609</td>\n      <td>11220.959961</td>\n      <td>198300000</td>\n      <td>11220.959961</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2008-09-04</td>\n      <td>11532.480469</td>\n      <td>11532.480469</td>\n      <td>11176.019531</td>\n      <td>11188.230469</td>\n      <td>229200000</td>\n      <td>11188.230469</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2008-09-03</td>\n      <td>11506.009766</td>\n      <td>11554.379883</td>\n      <td>11416.530273</td>\n      <td>11532.879883</td>\n      <td>174250000</td>\n      <td>11532.879883</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2008-09-02</td>\n      <td>11545.629883</td>\n      <td>11790.169922</td>\n      <td>11471.900391</td>\n      <td>11516.919922</td>\n      <td>177090000</td>\n      <td>11516.919922</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2008-08-29</td>\n      <td>11713.230469</td>\n      <td>11713.230469</td>\n      <td>11543.389648</td>\n      <td>11543.959961</td>\n      <td>166910000</td>\n      <td>11543.959961</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2008-08-28</td>\n      <td>11499.870117</td>\n      <td>11715.179688</td>\n      <td>11499.790039</td>\n      <td>11715.179688</td>\n      <td>149150000</td>\n      <td>11715.179688</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2008-08-27</td>\n      <td>11412.459961</td>\n      <td>11554.459961</td>\n      <td>11381.769531</td>\n      <td>11502.509766</td>\n      <td>120580000</td>\n      <td>11502.509766</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2008-08-26</td>\n      <td>11383.559570</td>\n      <td>11436.240234</td>\n      <td>11340.410156</td>\n      <td>11412.870117</td>\n      <td>119800000</td>\n      <td>11412.870117</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2008-08-25</td>\n      <td>11626.190430</td>\n      <td>11626.269531</td>\n      <td>11362.629883</td>\n      <td>11386.250000</td>\n      <td>148610000</td>\n      <td>11386.250000</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <td>2008-08-22</td>\n      <td>11426.790039</td>\n      <td>11632.129883</td>\n      <td>11426.790039</td>\n      <td>11628.059570</td>\n      <td>138790000</td>\n      <td>11628.059570</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2008-08-21</td>\n      <td>11415.230469</td>\n      <td>11476.209961</td>\n      <td>11315.570312</td>\n      <td>11430.209961</td>\n      <td>130020000</td>\n      <td>11430.209961</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2008-08-20</td>\n      <td>11345.940430</td>\n      <td>11454.150391</td>\n      <td>11290.580078</td>\n      <td>11417.429688</td>\n      <td>144880000</td>\n      <td>11417.429688</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2008-08-19</td>\n      <td>11478.089844</td>\n      <td>11478.169922</td>\n      <td>11318.500000</td>\n      <td>11348.549805</td>\n      <td>171580000</td>\n      <td>11348.549805</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2008-08-18</td>\n      <td>11659.650391</td>\n      <td>11690.429688</td>\n      <td>11434.120117</td>\n      <td>11479.389648</td>\n      <td>156290000</td>\n      <td>11479.389648</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2008-08-15</td>\n      <td>11611.209961</td>\n      <td>11709.889648</td>\n      <td>11599.730469</td>\n      <td>11659.900391</td>\n      <td>215040000</td>\n      <td>11659.900391</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2008-08-14</td>\n      <td>11532.070312</td>\n      <td>11718.280273</td>\n      <td>11450.889648</td>\n      <td>11615.929688</td>\n      <td>159790000</td>\n      <td>11615.929688</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2008-08-13</td>\n      <td>11632.809570</td>\n      <td>11633.780273</td>\n      <td>11453.339844</td>\n      <td>11532.959961</td>\n      <td>182550000</td>\n      <td>11532.959961</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2008-08-12</td>\n      <td>11781.700195</td>\n      <td>11782.349609</td>\n      <td>11601.519531</td>\n      <td>11642.469727</td>\n      <td>173590000</td>\n      <td>11642.469727</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2008-08-11</td>\n      <td>11729.669922</td>\n      <td>11867.110352</td>\n      <td>11675.530273</td>\n      <td>11782.349609</td>\n      <td>183190000</td>\n      <td>11782.349609</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2008-08-08</td>\n      <td>11432.089844</td>\n      <td>11759.959961</td>\n      <td>11388.040039</td>\n      <td>11734.320312</td>\n      <td>212830000</td>\n      <td>11734.320312</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "stock_df['Label'] = label_lst\n",
        "stock_df.tail(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wJuVxk2xWkL"
      },
      "source": [
        "## 2-2. Map your label to news data (20%)"
      ]
    },
    {
      "source": [
        "- join dataframes https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html\n",
        "- fillna https://www.geeksforgeeks.org/replace-nan-values-with-zeros-in-pandas-dataframe/"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djOnaxddkPFc"
      },
      "source": [
        "def news_label(news_df, stock_df):\n",
        "  '''\n",
        "  Map your close value trend label in news data by date, also name it \"Label\"\n",
        "  if a date does not appear in the stock data, also label it as -1.\n",
        "  e.g. all news in 2016-06-30 will be label -1\n",
        "  '''\n",
        "  # Create new dataframe with labels\n",
        "  df = news_df.join(stock_df, on='Date')\n",
        "  \n",
        "  # Apply the function \n",
        "  df['Label'] = df['Label'].fillna(-1) \n",
        "  return df[[\"Date\", \"News\", \"Label\"]]\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Date                                               News  Label\n",
              "0 2016-07-01  A 117-year-old woman in Mexico City finally re...   -1.0\n",
              "1 2016-07-01   IMF chief backs Athens as permanent Olympic host   -1.0\n",
              "2 2016-07-01  The president of France says if Brexit won, so...   -1.0\n",
              "3 2016-07-01  British Man Who Must Give Police 24 Hours' Not...   -1.0\n",
              "4 2016-07-01  100+ Nobel laureates urge Greenpeace to stop o...   -1.0\n",
              "5 2016-07-01  Brazil: Huge spike in number of police killing...   -1.0\n",
              "6 2016-07-01  Austria's highest court annuls presidential el...   -1.0\n",
              "7 2016-07-01  Facebook wins privacy case, can track any Belg...   -1.0\n",
              "8 2016-07-01  Switzerland denies Muslim girls citizenship af...   -1.0\n",
              "9 2016-07-01  China kills millions of innocent meditators fo...   -1.0"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>News</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2016-07-01</td>\n      <td>A 117-year-old woman in Mexico City finally re...</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2016-07-01</td>\n      <td>IMF chief backs Athens as permanent Olympic host</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2016-07-01</td>\n      <td>The president of France says if Brexit won, so...</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2016-07-01</td>\n      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2016-07-01</td>\n      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2016-07-01</td>\n      <td>Brazil: Huge spike in number of police killing...</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2016-07-01</td>\n      <td>Austria's highest court annuls presidential el...</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2016-07-01</td>\n      <td>Facebook wins privacy case, can track any Belg...</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2016-07-01</td>\n      <td>Switzerland denies Muslim girls citizenship af...</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2016-07-01</td>\n      <td>China kills millions of innocent meditators fo...</td>\n      <td>-1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "res = news_label(news_df, stock_df)\n",
        "res.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Date                                               News  Label\n",
              "73598 2008-06-08  b\"S. Korean protesters, police clash in beef r...   -1.0\n",
              "73599 2008-06-08  b\"Oil reserves 'will last decades' - a BBC Sco...   -1.0\n",
              "73600 2008-06-08  b'Cameras designed to detect terrorist facial ...   -1.0\n",
              "73601 2008-06-08  b'Israeli peace activists protest 41 years of ...   -1.0\n",
              "73602 2008-06-08  b\"A 5.1 earthquake hits China's Southern Qingh...   -1.0\n",
              "73603 2008-06-08  b'Man goes berzerk in Akihabara and stabs ever...   -1.0\n",
              "73604 2008-06-08  b'Threat of world AIDS pandemic among heterose...   -1.0\n",
              "73605 2008-06-08  b'Angst in Ankara: Turkey Steers into a Danger...   -1.0\n",
              "73606 2008-06-08  b\"UK: Identity cards 'could be used to spy on ...   -1.0\n",
              "73607 2008-06-08  b'Marriage, they said, was reduced to the stat...   -1.0"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>News</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>73598</td>\n      <td>2008-06-08</td>\n      <td>b\"S. Korean protesters, police clash in beef r...</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <td>73599</td>\n      <td>2008-06-08</td>\n      <td>b\"Oil reserves 'will last decades' - a BBC Sco...</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <td>73600</td>\n      <td>2008-06-08</td>\n      <td>b'Cameras designed to detect terrorist facial ...</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <td>73601</td>\n      <td>2008-06-08</td>\n      <td>b'Israeli peace activists protest 41 years of ...</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <td>73602</td>\n      <td>2008-06-08</td>\n      <td>b\"A 5.1 earthquake hits China's Southern Qingh...</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <td>73603</td>\n      <td>2008-06-08</td>\n      <td>b'Man goes berzerk in Akihabara and stabs ever...</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <td>73604</td>\n      <td>2008-06-08</td>\n      <td>b'Threat of world AIDS pandemic among heterose...</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <td>73605</td>\n      <td>2008-06-08</td>\n      <td>b'Angst in Ankara: Turkey Steers into a Danger...</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <td>73606</td>\n      <td>2008-06-08</td>\n      <td>b\"UK: Identity cards 'could be used to spy on ...</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <td>73607</td>\n      <td>2008-06-08</td>\n      <td>b'Marriage, they said, was reduced to the stat...</td>\n      <td>-1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "res.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Date                                               News  Label\n",
              "200 2016-06-23  Today The United Kingdom decides whether to re...    0.0\n",
              "201 2016-06-23  E-cigarettes should not be banned in public, m...    0.0\n",
              "202 2016-06-23  Report: China is still harvesting organs from ...    0.0\n",
              "203 2016-06-23  Man opens fire at cinema complex in Germany, s...    0.0\n",
              "204 2016-06-23  Erdoan: Europe, you dont want us because were ...    0.0\n",
              "205 2016-06-23  Asian millionaires now control more wealth tha...    0.0\n",
              "206 2016-06-23  A Japanese porn industry association has apolo...    0.0\n",
              "207 2016-06-23  University students are being warned when clas...    0.0\n",
              "208 2016-06-23        Afghan interpreters 'betrayed' by UK and US    0.0\n",
              "209 2016-06-23  Contagious cancer cells are spreading between ...    0.0"
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>News</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>2016-06-23</td>\n      <td>Today The United Kingdom decides whether to re...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>201</td>\n      <td>2016-06-23</td>\n      <td>E-cigarettes should not be banned in public, m...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>202</td>\n      <td>2016-06-23</td>\n      <td>Report: China is still harvesting organs from ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>203</td>\n      <td>2016-06-23</td>\n      <td>Man opens fire at cinema complex in Germany, s...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>204</td>\n      <td>2016-06-23</td>\n      <td>Erdoan: Europe, you dont want us because were ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>2016-06-23</td>\n      <td>Asian millionaires now control more wealth tha...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>206</td>\n      <td>2016-06-23</td>\n      <td>A Japanese porn industry association has apolo...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>207</td>\n      <td>2016-06-23</td>\n      <td>University students are being warned when clas...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>208</td>\n      <td>2016-06-23</td>\n      <td>Afghan interpreters 'betrayed' by UK and US</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>209</td>\n      <td>2016-06-23</td>\n      <td>Contagious cancer cells are spreading between ...</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "res.iloc[200:210]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1s56c_DiAhd"
      },
      "source": [
        "# Question 3: Compute cosine similarity of TF-IDF (term frequency–inverse document frequency)\r\n",
        "-  **Cosine similarity** is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. \r\n",
        "- **TF-IDF** is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQbAzTgLlqp0"
      },
      "source": [
        "## 3-1. Please answer why we can use cosine similarity to measure TF-IDF representation. Is there any other representation methods also can be measured by cosine similarity? (20%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAx5GYvJlsLU"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0FYrALsls2z"
      },
      "source": [
        "## 3-2. Define a converting function to compute tf-idf vector from a list of ducoments. (40%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLtcko0fkTIz"
      },
      "source": [
        "documents = ['terrible service this time','terrible terrible service','most terrible service','terrible service and experience','what a terrible service','so terrible service experience','what a terrible disappointment','what a terrible place','this time it was so horrible','the staff was horrible']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi1J_bODkT0e"
      },
      "source": [
        "'''\r\n",
        "Answer here\r\n",
        "you can define other functions to support the defined function if you need.\r\n",
        "\r\n",
        "TF-IDF dataframe show as the following table.\r\n",
        "'''\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "def computTFIDF(documents):\r\n",
        "\r\n",
        "  return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGLQEuX8kU20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "9817072f-5015-47d7-84a0-6cbe0038b63c"
      },
      "source": [
        "import pandas as pd\r\n",
        "tf_idf_list = computTFIDF(documents)\r\n",
        "df = pd.DataFrame(computTFIDF(documents))\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>terrible</th>\n",
              "      <th>service</th>\n",
              "      <th>this</th>\n",
              "      <th>time</th>\n",
              "      <th>most</th>\n",
              "      <th>and</th>\n",
              "      <th>experience</th>\n",
              "      <th>what</th>\n",
              "      <th>a</th>\n",
              "      <th>so</th>\n",
              "      <th>disappointment</th>\n",
              "      <th>place</th>\n",
              "      <th>it</th>\n",
              "      <th>was</th>\n",
              "      <th>horrible</th>\n",
              "      <th>the</th>\n",
              "      <th>staff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.024228</td>\n",
              "      <td>0.055462</td>\n",
              "      <td>0.174743</td>\n",
              "      <td>0.174743</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.064607</td>\n",
              "      <td>0.073950</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.032303</td>\n",
              "      <td>0.073950</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.024228</td>\n",
              "      <td>0.055462</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.174743</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.024228</td>\n",
              "      <td>0.055462</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.13072</td>\n",
              "      <td>0.13072</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.024228</td>\n",
              "      <td>0.055462</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.174743</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.174743</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.024228</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.13072</td>\n",
              "      <td>0.13072</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.024228</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.13072</td>\n",
              "      <td>0.13072</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.116495</td>\n",
              "      <td>0.116495</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.116495</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.116495</td>\n",
              "      <td>0.116495</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.174743</td>\n",
              "      <td>0.174743</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   terrible   service      this      time  ...       was  horrible   the  staff\n",
              "0  0.024228  0.055462  0.174743  0.174743  ...  0.000000  0.000000  0.00   0.00\n",
              "1  0.064607  0.073950  0.000000  0.000000  ...  0.000000  0.000000  0.00   0.00\n",
              "2  0.032303  0.073950  0.000000  0.000000  ...  0.000000  0.000000  0.00   0.00\n",
              "3  0.024228  0.055462  0.000000  0.000000  ...  0.000000  0.000000  0.00   0.00\n",
              "4  0.024228  0.055462  0.000000  0.000000  ...  0.000000  0.000000  0.00   0.00\n",
              "5  0.024228  0.055462  0.000000  0.000000  ...  0.000000  0.000000  0.00   0.00\n",
              "6  0.024228  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00   0.00\n",
              "7  0.024228  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00   0.00\n",
              "8  0.000000  0.000000  0.116495  0.116495  ...  0.116495  0.116495  0.00   0.00\n",
              "9  0.000000  0.000000  0.000000  0.000000  ...  0.174743  0.174743  0.25   0.25\n",
              "\n",
              "[10 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26rxvr4fqMRo"
      },
      "source": [
        "## 3-3. Define a scoring function to compute the cosine similarity between two input vector. (30%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIv3hr1MkVt0"
      },
      "source": [
        "'''\r\n",
        "Answer here\r\n",
        "return the cosine similarity between the given two vectors\r\n",
        "Apply the function which you designed to all sentences, and show your scoring results as the following table.\r\n",
        "'''\r\n",
        "def cosine_sim(vec_a, vec_b):\r\n",
        "\r\n",
        "  return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV2SxmzWqRF3"
      },
      "source": [
        "## 3-4. Show the cross comparation table for the given sentences. (10%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TYdcePcqcCB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "6f1c123f-adf8-4efa-d279-9a8de370c004"
      },
      "source": [],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.226813</td>\n",
              "      <td>0.055972</td>\n",
              "      <td>0.046299</td>\n",
              "      <td>0.074014</td>\n",
              "      <td>0.056587</td>\n",
              "      <td>0.007397</td>\n",
              "      <td>0.007397</td>\n",
              "      <td>0.517451</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.226813</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.224349</td>\n",
              "      <td>0.185576</td>\n",
              "      <td>0.296664</td>\n",
              "      <td>0.226813</td>\n",
              "      <td>0.051111</td>\n",
              "      <td>0.051111</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.055972</td>\n",
              "      <td>0.224349</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.045796</td>\n",
              "      <td>0.073209</td>\n",
              "      <td>0.055972</td>\n",
              "      <td>0.007317</td>\n",
              "      <td>0.007317</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.046299</td>\n",
              "      <td>0.185576</td>\n",
              "      <td>0.045796</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.060557</td>\n",
              "      <td>0.432244</td>\n",
              "      <td>0.006053</td>\n",
              "      <td>0.006053</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.074014</td>\n",
              "      <td>0.296664</td>\n",
              "      <td>0.073209</td>\n",
              "      <td>0.060557</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.074014</td>\n",
              "      <td>0.573020</td>\n",
              "      <td>0.573020</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.056587</td>\n",
              "      <td>0.226813</td>\n",
              "      <td>0.055972</td>\n",
              "      <td>0.432244</td>\n",
              "      <td>0.074014</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.007397</td>\n",
              "      <td>0.007397</td>\n",
              "      <td>0.258725</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.007397</td>\n",
              "      <td>0.051111</td>\n",
              "      <td>0.007317</td>\n",
              "      <td>0.006053</td>\n",
              "      <td>0.573020</td>\n",
              "      <td>0.007397</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.357407</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.007397</td>\n",
              "      <td>0.051111</td>\n",
              "      <td>0.007317</td>\n",
              "      <td>0.006053</td>\n",
              "      <td>0.573020</td>\n",
              "      <td>0.007397</td>\n",
              "      <td>0.357407</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.517451</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.258725</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.305206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.305206</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2  ...         7         8         9\n",
              "0  1.000000  0.226813  0.055972  ...  0.007397  0.517451  0.000000\n",
              "1  0.226813  1.000000  0.224349  ...  0.051111  0.000000  0.000000\n",
              "2  0.055972  0.224349  1.000000  ...  0.007317  0.000000  0.000000\n",
              "3  0.046299  0.185576  0.045796  ...  0.006053  0.000000  0.000000\n",
              "4  0.074014  0.296664  0.073209  ...  0.573020  0.000000  0.000000\n",
              "5  0.056587  0.226813  0.055972  ...  0.007397  0.258725  0.000000\n",
              "6  0.007397  0.051111  0.007317  ...  0.357407  0.000000  0.000000\n",
              "7  0.007397  0.051111  0.007317  ...  1.000000  0.000000  0.000000\n",
              "8  0.517451  0.000000  0.000000  ...  0.000000  1.000000  0.305206\n",
              "9  0.000000  0.000000  0.000000  ...  0.000000  0.305206  1.000000\n",
              "\n",
              "[10 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}