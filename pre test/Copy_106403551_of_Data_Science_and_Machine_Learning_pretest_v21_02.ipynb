{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy 106403551 of Data Science and Machine Learning pretest v21.02.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4MpIDbALpZ9"
      },
      "source": [
        "# 2021 Data Science and Machine Learning Pretest\r\n",
        "\r\n",
        "*   This Colab is read-only. Please save a copy of it on your Drive to edit it by going to `Menu > File > Save a copy in Drive`.\r\n",
        "*   Rename your Colab in the following format and replace 109423000 with your student ID:\r\n",
        "> `Copy 109423000 of Data Science and Machine Learning pretest v21.02.ipynb`\r\n",
        "*   You are required to complete this pretest **on your own**.\r\n",
        "*   When you have completed the questions below, make sure you turn on the **share/edit/view persmission**.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOY37mmch2f9"
      },
      "source": [
        "# Question 1: Text preprocessing\n",
        "Most of the text data acquired through web crawling and review can be noisy. When handling this kind of text data, preprocessing is an important step to ensure the quality of the dataset. There are multiple ways of doing text preprocessing. Below is an example flow of preprocessing text data. \n",
        "\n",
        "1. lowercase \n",
        "2. decontracting \n",
        "3. remove tags, punctuations, numbers\n",
        "3. tokenization \n",
        "4. stopword removal \n",
        "5. lemmatization \n",
        "6. stemming\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "source": [
        "## 1-1. Please briefly explain what each step is doing.(30%)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "\n",
        "1. Standardize text data in single format. Since the word 'love' is same as 'LoVE'.\n",
        "2. English has a couple of contractions(abbreviations/縮寫). For instance 'aren't' stands for 'are not'. We should split/decontract contractions in full sentence in case of consistency(一致性).\n",
        "3. In many cases, we want to remove all the non-words charactersthe (e.g. punctuation marks, HTML tags) and it’s easy to remove them with regex.\n",
        "4. Since the data all we have are sentences. But the basic document unit are 'tokens', not 'sentences'. A *token* is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing.\n",
        "    - *Sentence tokenization* (also called sentence segmentation) is the problem of dividing a string of written language into its component sentences.\n",
        "    - *Word tokenization* (also called word segmentation) is the problem of dividing a string of written language into its component words.\n",
        "5. Stop words are words which are **filtered out** before or after processing of text. They are the words in any language which does not add much meaning to a sentence. Stopwords could cause noise, that’s why we want to remove these irrelevant words. Common stop words: “and”, “the”, “a”\n",
        "\n",
        "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:\n",
        "```js\n",
        "am, are, is => be //統一使用字根\n",
        "car, cars, car's, cars' => car //去詞類變化\n",
        "```\n",
        "The result of this mapping of text will be something like:\n",
        "```\n",
        "the boy's cars are different colors =>\n",
        "the boy car be differ color\n",
        "```\n",
        "\n",
        "6. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words(基於語言學/字典定義), normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .\n",
        "7. Stemmers use language-specific rules, but they require less knowledge than a lemmatizer. It usually refers to a crude heuristic process(猜字根) that chops off the ends of words, which increases recall while harming precision. \n",
        "\n",
        "References:\n",
        "- https://nlp.stanford.edu/IR-book/html/htmledition/contents-1.html\n",
        "- https://towardsdatascience.com/introduction-to-natural-language-processing-for-text-df845750fb63\n",
        "- https://medium.com/analytics-vidhya/natural-language-processing-for-developers-912ee0fda979 (with github code)\n",
        "- Tokenization https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/\n",
        "- Stop words https://medium.com/@saitejaponugoti/stop-words-in-nlp-5b248dadad47#:~:text=In%20computing%2C%20stop%20words%20are,universal%20list%20of%20stop%20words.\n",
        "- [Youtube | Natural Language Processing In 10 Minutes](https://www.youtube.com/watch?v=5ctbvkAMQO4)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP3t4W6flEIC"
      },
      "source": [
        "## 1-2. Please use the sample data and do the preprocessing following the provided flow.(70%)"
      ]
    },
    {
      "source": [
        "documents = [\"Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as 'Jumbo'\",\r\n",
        "        \"I WAS VISITING MY FRIEND NATE THE OTHER MORNING FOR COFFEE , HE CAME OUT OF HIS STORAGE ROOM WITH ( A PACKET OF McCANNS INSTANT IRISH OATMEAL .) HE SUGGESTED THAT I TRY IT FOR MY OWN USE ,IN MY STASH . SOMETIMES NATE DOSE NOT GIVE YOU A CHANCE TO SAY NO , SO I ENDED UP TRYING THE APPLE AND CINN . FOUND IT TO BE VERY TASTEFULL WHEN MADE WITH WATER OR POWDERED MILK . IT GOES GOOD WITH O.J. AND COFFEE AND A SLICE OF TOAST AND YOUR READY TO TAKE ON THE WORLD...OR THE DAY AT LEAST..  JERRY REITH...\",\r\n",
        "        \"I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind!  We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away!  When we realized that we simply couldn't find it anywhere in our city we were bummed.<br /><br />Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.<br /><br />If you love hot sauce..I mean really love hot sauce, but don't want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan.  Just realize that once you taste it, you will never want to use any other sauce.<br /><br />Thank you for the personal, incredible service!\",\r\n",
        "        \"Product received is as advertised.<br /><br /><a href='http://www.amazon.com/gp/product/B001GVISJM'>Twizzlers, Strawberry, 16-Ounce Bags (Pack of 6)</a>\",\r\n",
        "        \"this was sooooo deliscious but too bad i ate em too fast and gained 2 pds! my fault\",\r\n",
        "        \"Deal was awesome!  Arrived before Halloween as indicated and was enough to satisfy trick or treaters.  I love the quality of this product and it was much less expensive than the local store's candy.\",\r\n",
        "        \"I love these.........very tasty!!!!!!!!!!!  Infact, I think I am addicted to them.<br />Buying them in packs of 6 bags - is very reasonable than going to Target and getting a bag.  Savings are about a $1.00 a bag.  I use subscribe and save on these and several other product.  I love subscribe and save!!!!!!!!!!!\",\r\n",
        "        \"I LOVE spicy ramen, but for whatever reasons this thing burns my stomach badly and the burning sensation doesn't go away for like 3 hours! Not sure if that is healthy or not .... and you can buy this at Walmart for $0.28, way cheaper than Amazon.\",\r\n",
        "        \"Makes a tasty, super easy meal, fast. BUT high in calories.<br /><br />The instructions say to saute the veggies first but I recommend cooking the chicken first. The chicken takes longer to cook and the raw chicken ontop of veggies just makes a slimy mess. I made it with snow peas and carrots only. I dont like the little corn.  Added some red pepper flakes for heat and served ontop of rice.  It came out wonderful! Dinner on the table in less than 30mins.\",\r\n",
        "        \"Love this sugar.  I also get muscavado sugar and they are both great to use in place of regular white sugar. Recommend!\",\r\n",
        "        \"This is just Fantastic Chicken Noodle soup, the best I have ever eaten, with large hearty chunks of chicken,and vegetables and nice large noodles. This soup is just so full bodied, and is seasoned just right.  I am so glad Amazon carries this product.  I just can't find it here in Vermont.\"]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "y7n8-qVplIrU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "LLEiWlmoqmpY",
        "outputId": "b3d26426-9cc0-4241-df7b-edc17b98e7f5"
      },
      "source": [
        "import pandas as pd\r\n",
        "df= pd.DataFrame(documents,columns=['sentence'])\r\n",
        "df"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             sentence\n",
              "0   Product arrived labeled as Jumbo Salted Peanut...\n",
              "1   I WAS VISITING MY FRIEND NATE THE OTHER MORNIN...\n",
              "2   I don't know if it's the cactus or the tequila...\n",
              "3   Product received is as advertised.<br /><br />...\n",
              "4   this was sooooo deliscious but too bad i ate e...\n",
              "5   Deal was awesome!  Arrived before Halloween as...\n",
              "6   I love these.........very tasty!!!!!!!!!!!  In...\n",
              "7   I LOVE spicy ramen, but for whatever reasons t...\n",
              "8   Makes a tasty, super easy meal, fast. BUT high...\n",
              "9   Love this sugar.  I also get muscavado sugar a...\n",
              "10  This is just Fantastic Chicken Noodle soup, th..."
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>I WAS VISITING MY FRIEND NATE THE OTHER MORNIN...</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>I don't know if it's the cactus or the tequila...</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>Product received is as advertised.&lt;br /&gt;&lt;br /&gt;...</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>this was sooooo deliscious but too bad i ate e...</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>Deal was awesome!  Arrived before Halloween as...</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>I love these.........very tasty!!!!!!!!!!!  In...</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>I LOVE spicy ramen, but for whatever reasons t...</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>Makes a tasty, super easy meal, fast. BUT high...</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>Love this sugar.  I also get muscavado sugar a...</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>This is just Fantastic Chicken Noodle soup, th...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Weber\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# decontracting\n",
        "def decontract(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "# remove non-alpha/number charater (Punctuation Marks)\n",
        "def text(phrase):\n",
        "    # By Python definition '\\W == [^a-zA-Z0-9_], which excludes all numbers, letters and _ (but we need to include whitespace \\s)\n",
        "    phrase = re.sub(r'[^a-zA-Z0-9_\\s]+', '', phrase)\n",
        "    return phrase\n",
        "\n",
        "# stopword removal & tokenization\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "# nltk.download('stopwords') # must download at first execution\n",
        "# nltk.download('punkt')\n",
        "\n",
        "def tokenize_rm_stopwords(phrase): \n",
        "    text_tokens = word_tokenize(phrase)\n",
        "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('english')]\n",
        "    return tokens_without_sw\n",
        "\n",
        "# stem/lemmatize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "# nltk.download('wordnet') \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize(token_lst):\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in token_lst])\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh man this is pretty cool We will do more such things\n['Oh', 'man', 'pretty', 'cool', 'We', 'things']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Oh man pretty cool We thing'"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# Unit test\n",
        "sample_text = \"Oh man, this's pretty cool. We will do more such things.\"\n",
        "res = decontract(sample_text)\n",
        "res = text(res)\n",
        "print(res)\n",
        "res = tokenize_rm_stopwords(res) # ㄚㄚㄚ砍太多字惹吧\n",
        "print(res)\n",
        "res = lemmatize(res)\n",
        "\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# modulize, can't change order\n",
        "def process_script(phrase):\n",
        "    phrase = phrase.lower()\n",
        "    res = decontract(phrase)\n",
        "    res = text(res)\n",
        "    res = tokenize_rm_stopwords(res)\n",
        "    res = lemmatize(res)\n",
        "    return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             sentence  \\\n",
              "0   Product arrived labeled as Jumbo Salted Peanut...   \n",
              "1   I WAS VISITING MY FRIEND NATE THE OTHER MORNIN...   \n",
              "2   I don't know if it's the cactus or the tequila...   \n",
              "3   Product received is as advertised.<br /><br />...   \n",
              "4   this was sooooo deliscious but too bad i ate e...   \n",
              "5   Deal was awesome!  Arrived before Halloween as...   \n",
              "6   I love these.........very tasty!!!!!!!!!!!  In...   \n",
              "7   I LOVE spicy ramen, but for whatever reasons t...   \n",
              "8   Makes a tasty, super easy meal, fast. BUT high...   \n",
              "9   Love this sugar.  I also get muscavado sugar a...   \n",
              "10  This is just Fantastic Chicken Noodle soup, th...   \n",
              "\n",
              "                                            processed  \n",
              "0   product arrived labeled jumbo salted peanutsth...  \n",
              "1   visiting friend nate morning coffee came stora...  \n",
              "2   know cactus tequila unique combination ingredi...  \n",
              "3   product received advertisedbr br hrefhttpwwwam...  \n",
              "4   sooooo deliscious bad ate em fast gained 2 pd ...  \n",
              "5   deal awesome arrived halloween indicated enoug...  \n",
              "6   love thesevery tasty infact think addicted the...  \n",
              "7   love spicy ramen whatever reason thing burn st...  \n",
              "8   make tasty super easy meal fast high caloriesb...  \n",
              "9   love sugar also get muscavado sugar great use ...  \n",
              "10  fantastic chicken noodle soup best ever eaten ...  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>processed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n      <td>product arrived labeled jumbo salted peanutsth...</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>I WAS VISITING MY FRIEND NATE THE OTHER MORNIN...</td>\n      <td>visiting friend nate morning coffee came stora...</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>I don't know if it's the cactus or the tequila...</td>\n      <td>know cactus tequila unique combination ingredi...</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>Product received is as advertised.&lt;br /&gt;&lt;br /&gt;...</td>\n      <td>product received advertisedbr br hrefhttpwwwam...</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>this was sooooo deliscious but too bad i ate e...</td>\n      <td>sooooo deliscious bad ate em fast gained 2 pd ...</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>Deal was awesome!  Arrived before Halloween as...</td>\n      <td>deal awesome arrived halloween indicated enoug...</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>I love these.........very tasty!!!!!!!!!!!  In...</td>\n      <td>love thesevery tasty infact think addicted the...</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>I LOVE spicy ramen, but for whatever reasons t...</td>\n      <td>love spicy ramen whatever reason thing burn st...</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>Makes a tasty, super easy meal, fast. BUT high...</td>\n      <td>make tasty super easy meal fast high caloriesb...</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>Love this sugar.  I also get muscavado sugar a...</td>\n      <td>love sugar also get muscavado sugar great use ...</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>This is just Fantastic Chicken Noodle soup, th...</td>\n      <td>fantastic chicken noodle soup best ever eaten ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# show results\n",
        "df['processed'] = df.apply(lambda row: process_script(row['sentence']), axis = 1)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a8slpNZh_ID"
      },
      "source": [
        "# Question 2: DataFrame handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGkOfX9LiF4o"
      },
      "source": [
        "\r\n",
        "\r\n",
        "*   Please download the datasets from the following link, https://www.kaggle.com/aaron7sun/stocknews\r\n",
        "*   Save the downloaded files on your own drive and load the file for later use.\r\n",
        "- There are two channels of data provided in this dataset:\r\n",
        "\r\n",
        "  - **News data:** Crawled historical news headlines from Reddit WorldNews Channel . They are ranked by reddit users' votes, and only the top 25 headlines are considered for a single date. (Range: 2008-06-08 to 2016-07-01)\r\n",
        "\r\n",
        "  - **Stock data:** Dow Jones Industrial Average (DJIA) is used to \"prove the concept\". (Range: 2008-08-08 to 2016-07-01)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaguSDD3iO_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1477f364-62ce-4d94-b96d-782118f8778d"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX970ixtiPnh"
      },
      "source": [
        "import pandas as pd\r\n",
        "corpus_root = 'drive/My Drive/Colab Notebooks/datasets/' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHMR3Rv5iRV4"
      },
      "source": [
        "news_df = pd.read_csv(corpus_root+'RedditNews.csv')\r\n",
        "stock_df = pd.read_csv(corpus_root+'upload_DJIA_table.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFqPUNxWkK-K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4b86fde3-3c17-45bf-8797-ad31877db4cc"
      },
      "source": [
        "news_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>News</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>The president of France says if Brexit won, so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date                                               News\n",
              "0  2016-07-01  A 117-year-old woman in Mexico City finally re...\n",
              "1  2016-07-01   IMF chief backs Athens as permanent Olympic host\n",
              "2  2016-07-01  The president of France says if Brexit won, so...\n",
              "3  2016-07-01  British Man Who Must Give Police 24 Hours' Not...\n",
              "4  2016-07-01  100+ Nobel laureates urge Greenpeace to stop o..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smpTiq1DiST4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "6093d5c4-cd49-42e0-cb65-ebfbe47b81e5"
      },
      "source": [
        "stock_df.head(15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Adj Close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>17924.240234</td>\n",
              "      <td>18002.380859</td>\n",
              "      <td>17916.910156</td>\n",
              "      <td>17949.369141</td>\n",
              "      <td>82160000</td>\n",
              "      <td>17949.369141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-06-30</td>\n",
              "      <td>17712.759766</td>\n",
              "      <td>17930.609375</td>\n",
              "      <td>17711.800781</td>\n",
              "      <td>17929.990234</td>\n",
              "      <td>133030000</td>\n",
              "      <td>17929.990234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-06-29</td>\n",
              "      <td>17456.019531</td>\n",
              "      <td>17704.509766</td>\n",
              "      <td>17456.019531</td>\n",
              "      <td>17694.679688</td>\n",
              "      <td>106380000</td>\n",
              "      <td>17694.679688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-06-28</td>\n",
              "      <td>17190.509766</td>\n",
              "      <td>17409.720703</td>\n",
              "      <td>17190.509766</td>\n",
              "      <td>17409.720703</td>\n",
              "      <td>112190000</td>\n",
              "      <td>17409.720703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-06-27</td>\n",
              "      <td>17355.210938</td>\n",
              "      <td>17355.210938</td>\n",
              "      <td>17063.080078</td>\n",
              "      <td>17140.240234</td>\n",
              "      <td>138740000</td>\n",
              "      <td>17140.240234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2016-06-24</td>\n",
              "      <td>17946.630859</td>\n",
              "      <td>17946.630859</td>\n",
              "      <td>17356.339844</td>\n",
              "      <td>17400.750000</td>\n",
              "      <td>239000000</td>\n",
              "      <td>17400.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2016-06-23</td>\n",
              "      <td>17844.109375</td>\n",
              "      <td>18011.070312</td>\n",
              "      <td>17844.109375</td>\n",
              "      <td>18011.070312</td>\n",
              "      <td>98070000</td>\n",
              "      <td>18011.070312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2016-06-22</td>\n",
              "      <td>17832.669922</td>\n",
              "      <td>17920.160156</td>\n",
              "      <td>17770.359375</td>\n",
              "      <td>17780.830078</td>\n",
              "      <td>89440000</td>\n",
              "      <td>17780.830078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2016-06-21</td>\n",
              "      <td>17827.330078</td>\n",
              "      <td>17877.839844</td>\n",
              "      <td>17799.800781</td>\n",
              "      <td>17829.730469</td>\n",
              "      <td>85130000</td>\n",
              "      <td>17829.730469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2016-06-20</td>\n",
              "      <td>17736.869141</td>\n",
              "      <td>17946.359375</td>\n",
              "      <td>17736.869141</td>\n",
              "      <td>17804.869141</td>\n",
              "      <td>99380000</td>\n",
              "      <td>17804.869141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2016-06-17</td>\n",
              "      <td>17733.439453</td>\n",
              "      <td>17733.439453</td>\n",
              "      <td>17602.779297</td>\n",
              "      <td>17675.160156</td>\n",
              "      <td>248680000</td>\n",
              "      <td>17675.160156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2016-06-16</td>\n",
              "      <td>17602.230469</td>\n",
              "      <td>17754.910156</td>\n",
              "      <td>17471.289062</td>\n",
              "      <td>17733.099609</td>\n",
              "      <td>91950000</td>\n",
              "      <td>17733.099609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2016-06-15</td>\n",
              "      <td>17703.650391</td>\n",
              "      <td>17762.960938</td>\n",
              "      <td>17629.009766</td>\n",
              "      <td>17640.169922</td>\n",
              "      <td>94130000</td>\n",
              "      <td>17640.169922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2016-06-14</td>\n",
              "      <td>17710.769531</td>\n",
              "      <td>17733.919922</td>\n",
              "      <td>17595.789062</td>\n",
              "      <td>17674.820312</td>\n",
              "      <td>93740000</td>\n",
              "      <td>17674.820312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2016-06-13</td>\n",
              "      <td>17830.500000</td>\n",
              "      <td>17893.279297</td>\n",
              "      <td>17731.349609</td>\n",
              "      <td>17732.480469</td>\n",
              "      <td>101690000</td>\n",
              "      <td>17732.480469</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Date          Open  ...     Volume     Adj Close\n",
              "0   2016-07-01  17924.240234  ...   82160000  17949.369141\n",
              "1   2016-06-30  17712.759766  ...  133030000  17929.990234\n",
              "2   2016-06-29  17456.019531  ...  106380000  17694.679688\n",
              "3   2016-06-28  17190.509766  ...  112190000  17409.720703\n",
              "4   2016-06-27  17355.210938  ...  138740000  17140.240234\n",
              "5   2016-06-24  17946.630859  ...  239000000  17400.750000\n",
              "6   2016-06-23  17844.109375  ...   98070000  18011.070312\n",
              "7   2016-06-22  17832.669922  ...   89440000  17780.830078\n",
              "8   2016-06-21  17827.330078  ...   85130000  17829.730469\n",
              "9   2016-06-20  17736.869141  ...   99380000  17804.869141\n",
              "10  2016-06-17  17733.439453  ...  248680000  17675.160156\n",
              "11  2016-06-16  17602.230469  ...   91950000  17733.099609\n",
              "12  2016-06-15  17703.650391  ...   94130000  17640.169922\n",
              "13  2016-06-14  17710.769531  ...   93740000  17674.820312\n",
              "14  2016-06-13  17830.500000  ...  101690000  17732.480469\n",
              "\n",
              "[15 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H03CMsosjphW"
      },
      "source": [
        "\r\n",
        "## 2-1. Define a *funtion* aims to know the weekly (7 days) Stock **Close value** trend. (80%)\r\n",
        "- Implement a *funtion* that determines the weekly (7 days) Stock **Close value** trend using the Stock data and **then record the trend in the News data in a new column \"label\" with 0, 1 and -1**. For example, \r\n",
        "  - On 2016-06-13, the market closed at 17732.480469. Seven days later, 2016-06-20, the market closed **higher** at 17804.869141. In this scenario, all entries corresponding to 2016-06-13 in the News data will be **marked 1** in the \"label\" column. Dates 2016-06-14 and 2016-06-15 will also be marked 1 because 2016-06-21 and 2016-06-22 closed higher, respectively.\r\n",
        "  - On the other hand, 2016-06-17 will be **marked 0** because 2016-06-24 (7 days later) closed **lower**.\r\n",
        "  - If a given date does not have a corresponding date for 7 days later, the given date will be **marked -1**.\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yf_CKVXkNy8"
      },
      "source": [
        "import datetime\r\n",
        "def label():\r\n",
        "  '''\r\n",
        "  please answer here, you can add any parameters if you want.\r\n",
        "  but don't import other libraries, this notebook already prepared the libraries which all you need !\r\n",
        "  remember that, the standard for evaluation include your:\r\n",
        "    1. Time Complexity (80%)\r\n",
        "    2. Program Logic (10%)\r\n",
        "    3. Creativity (10%)\r\n",
        "  '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUM9RQjkkOIk"
      },
      "source": [
        "'''\r\n",
        "than apply your defined function on stock_df here.\r\n",
        "return results store into a new columns name \"Label\"\r\n",
        "\r\n",
        "'''\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wJuVxk2xWkL"
      },
      "source": [
        "## 2-2. Map your label to news data (20%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djOnaxddkPFc"
      },
      "source": [
        "def news_label():\r\n",
        "  '''\r\n",
        "  Map your close value trend label in news data by date, also name it \"Label\"\r\n",
        "  if a date does not appear in the stock data, also label it as -1.\r\n",
        "  e.g. all news in 2016-06-30 will be label -1\r\n",
        "  '''\r\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1s56c_DiAhd"
      },
      "source": [
        "# Question 3: Compute cosine similarity of TF-IDF (term frequency–inverse document frequency)\r\n",
        "-  **Cosine similarity** is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. \r\n",
        "- **TF-IDF** is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQbAzTgLlqp0"
      },
      "source": [
        "## 3-1. Please answer why we can use cosine similarity to measure TF-IDF representation. Is there any other representation methods also can be measured by cosine similarity? (20%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAx5GYvJlsLU"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0FYrALsls2z"
      },
      "source": [
        "## 3-2. Define a converting function to compute tf-idf vector from a list of ducoments. (40%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLtcko0fkTIz"
      },
      "source": [
        "documents = ['terrible service this time','terrible terrible service','most terrible service','terrible service and experience','what a terrible service','so terrible service experience','what a terrible disappointment','what a terrible place','this time it was so horrible','the staff was horrible']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi1J_bODkT0e"
      },
      "source": [
        "'''\r\n",
        "Answer here\r\n",
        "you can define other functions to support the defined function if you need.\r\n",
        "\r\n",
        "TF-IDF dataframe show as the following table.\r\n",
        "'''\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "def computTFIDF(documents):\r\n",
        "\r\n",
        "  return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGLQEuX8kU20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "9817072f-5015-47d7-84a0-6cbe0038b63c"
      },
      "source": [
        "import pandas as pd\r\n",
        "tf_idf_list = computTFIDF(documents)\r\n",
        "df = pd.DataFrame(computTFIDF(documents))\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>terrible</th>\n",
              "      <th>service</th>\n",
              "      <th>this</th>\n",
              "      <th>time</th>\n",
              "      <th>most</th>\n",
              "      <th>and</th>\n",
              "      <th>experience</th>\n",
              "      <th>what</th>\n",
              "      <th>a</th>\n",
              "      <th>so</th>\n",
              "      <th>disappointment</th>\n",
              "      <th>place</th>\n",
              "      <th>it</th>\n",
              "      <th>was</th>\n",
              "      <th>horrible</th>\n",
              "      <th>the</th>\n",
              "      <th>staff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.024228</td>\n",
              "      <td>0.055462</td>\n",
              "      <td>0.174743</td>\n",
              "      <td>0.174743</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.064607</td>\n",
              "      <td>0.073950</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.032303</td>\n",
              "      <td>0.073950</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.024228</td>\n",
              "      <td>0.055462</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.174743</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.024228</td>\n",
              "      <td>0.055462</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.13072</td>\n",
              "      <td>0.13072</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.024228</td>\n",
              "      <td>0.055462</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.174743</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.174743</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.024228</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.13072</td>\n",
              "      <td>0.13072</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.024228</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.13072</td>\n",
              "      <td>0.13072</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.116495</td>\n",
              "      <td>0.116495</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.116495</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.116495</td>\n",
              "      <td>0.116495</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.174743</td>\n",
              "      <td>0.174743</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   terrible   service      this      time  ...       was  horrible   the  staff\n",
              "0  0.024228  0.055462  0.174743  0.174743  ...  0.000000  0.000000  0.00   0.00\n",
              "1  0.064607  0.073950  0.000000  0.000000  ...  0.000000  0.000000  0.00   0.00\n",
              "2  0.032303  0.073950  0.000000  0.000000  ...  0.000000  0.000000  0.00   0.00\n",
              "3  0.024228  0.055462  0.000000  0.000000  ...  0.000000  0.000000  0.00   0.00\n",
              "4  0.024228  0.055462  0.000000  0.000000  ...  0.000000  0.000000  0.00   0.00\n",
              "5  0.024228  0.055462  0.000000  0.000000  ...  0.000000  0.000000  0.00   0.00\n",
              "6  0.024228  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00   0.00\n",
              "7  0.024228  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.00   0.00\n",
              "8  0.000000  0.000000  0.116495  0.116495  ...  0.116495  0.116495  0.00   0.00\n",
              "9  0.000000  0.000000  0.000000  0.000000  ...  0.174743  0.174743  0.25   0.25\n",
              "\n",
              "[10 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26rxvr4fqMRo"
      },
      "source": [
        "## 3-3. Define a scoring function to compute the cosine similarity between two input vector. (30%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIv3hr1MkVt0"
      },
      "source": [
        "'''\r\n",
        "Answer here\r\n",
        "return the cosine similarity between the given two vectors\r\n",
        "Apply the function which you designed to all sentences, and show your scoring results as the following table.\r\n",
        "'''\r\n",
        "def cosine_sim(vec_a, vec_b):\r\n",
        "\r\n",
        "  return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV2SxmzWqRF3"
      },
      "source": [
        "## 3-4. Show the cross comparation table for the given sentences. (10%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TYdcePcqcCB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "6f1c123f-adf8-4efa-d279-9a8de370c004"
      },
      "source": [],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.226813</td>\n",
              "      <td>0.055972</td>\n",
              "      <td>0.046299</td>\n",
              "      <td>0.074014</td>\n",
              "      <td>0.056587</td>\n",
              "      <td>0.007397</td>\n",
              "      <td>0.007397</td>\n",
              "      <td>0.517451</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.226813</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.224349</td>\n",
              "      <td>0.185576</td>\n",
              "      <td>0.296664</td>\n",
              "      <td>0.226813</td>\n",
              "      <td>0.051111</td>\n",
              "      <td>0.051111</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.055972</td>\n",
              "      <td>0.224349</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.045796</td>\n",
              "      <td>0.073209</td>\n",
              "      <td>0.055972</td>\n",
              "      <td>0.007317</td>\n",
              "      <td>0.007317</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.046299</td>\n",
              "      <td>0.185576</td>\n",
              "      <td>0.045796</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.060557</td>\n",
              "      <td>0.432244</td>\n",
              "      <td>0.006053</td>\n",
              "      <td>0.006053</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.074014</td>\n",
              "      <td>0.296664</td>\n",
              "      <td>0.073209</td>\n",
              "      <td>0.060557</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.074014</td>\n",
              "      <td>0.573020</td>\n",
              "      <td>0.573020</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.056587</td>\n",
              "      <td>0.226813</td>\n",
              "      <td>0.055972</td>\n",
              "      <td>0.432244</td>\n",
              "      <td>0.074014</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.007397</td>\n",
              "      <td>0.007397</td>\n",
              "      <td>0.258725</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.007397</td>\n",
              "      <td>0.051111</td>\n",
              "      <td>0.007317</td>\n",
              "      <td>0.006053</td>\n",
              "      <td>0.573020</td>\n",
              "      <td>0.007397</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.357407</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.007397</td>\n",
              "      <td>0.051111</td>\n",
              "      <td>0.007317</td>\n",
              "      <td>0.006053</td>\n",
              "      <td>0.573020</td>\n",
              "      <td>0.007397</td>\n",
              "      <td>0.357407</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.517451</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.258725</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.305206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.305206</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2  ...         7         8         9\n",
              "0  1.000000  0.226813  0.055972  ...  0.007397  0.517451  0.000000\n",
              "1  0.226813  1.000000  0.224349  ...  0.051111  0.000000  0.000000\n",
              "2  0.055972  0.224349  1.000000  ...  0.007317  0.000000  0.000000\n",
              "3  0.046299  0.185576  0.045796  ...  0.006053  0.000000  0.000000\n",
              "4  0.074014  0.296664  0.073209  ...  0.573020  0.000000  0.000000\n",
              "5  0.056587  0.226813  0.055972  ...  0.007397  0.258725  0.000000\n",
              "6  0.007397  0.051111  0.007317  ...  0.357407  0.000000  0.000000\n",
              "7  0.007397  0.051111  0.007317  ...  1.000000  0.000000  0.000000\n",
              "8  0.517451  0.000000  0.000000  ...  0.000000  1.000000  0.305206\n",
              "9  0.000000  0.000000  0.000000  ...  0.000000  0.305206  1.000000\n",
              "\n",
              "[10 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}